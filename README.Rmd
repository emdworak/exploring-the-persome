---
title: "Persome Analyses and Results"
author: "Elizabeth Dworak"
data: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  github_document:
      toc: true
  html_document:
    toc: true
    toc_float: true
    highlight: "tango"
---

```{r global-options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

# Overview
This repository includes the code provided in the appendix of [Revelle, Dworak, &amp; Condon (2021)](https://doi.org/10.1016/j.paid.2020.109905) As of "`r format(Sys.time(), '%d %B, %Y')`", this repository is a draft.


# A1. Reliability analyses for the SPI data set
```{r}
# Load the packages
library(psych) # make this package active
library(psychTools) # make this one active as well
```

For the purpose of making tables for github file, we need to load tidyverse to make tables more visually pleasing.
```{r}
library(tidyverse)
```


## Find alpha and omega for each scale
First find $\alpha$ and $\omega_{h}$ for each of the Big 5. Then find $\alpha$ for the little 27.
```{r}
omA <- omega(spi[selectFromKeys(spi.keys$Agree)],plot=FALSE)
omC <- omega(spi[selectFromKeys(spi.keys$Consc)],plot=FALSE)
omN <- omega(spi[selectFromKeys(spi.keys$Neuro)],plot=FALSE)
omE <- omega(spi[selectFromKeys(spi.keys$Extra)],plot=FALSE)
omO <- omega(spi[selectFromKeys(spi.keys$Open)],plot=FALSE)

omega.h <- c(omA$omega_h,omC$omega_h,omN$omega_h,omE$omega_h,omO$omega_h)
omega.t <- c(omA$omega.tot,omC$omega.tot,omN$omega.tot,omE$omega.tot,omO$omega.tot)
alphas <- c(omA$alpha,omC$alpha ,omN$alpha,omE$alpha,omO$alpha)

omega.df <- data.frame(omgega_total = omega.t,alpha=alphas,omega_h = omega.h)
rownames(omega.df) <- cs(Agreeableness, Conscientiousness, Neuroticism,
                         Extraversion, Openness)
```

## Find the scale scores for the Big 5 and little 27

```{r}
spi.scales <- scoreItems(spi.keys,spi) #find scores as well as scale statistics
spi.scores <- data.frame(spi[1:10],spi.scales$scores) #combine demographics and scores
R5<- lowerCor(spi.scores[11:15])
basic.stats <- cbind(omega.df,R5)
```

## Report all the values we just calculated.
This output corresponds with the results of Table 2 from [Revelle, Dworak, &amp; Condon (2021)](https://doi.org/10.1016/j.paid.2020.109905). 
```{r}
# This code matches what was used in the analyses for the paper. 
# round(basic.stats, 2) 

# This code is just a more visually appealing table of the code above
basic.stats %>% 
  round(., 2) %>% 
  knitr::kable(caption = "Reliabilities and correlations of the Big 5 scales from the `spi` data set. 
               Alpha reliability is on the diagonal of the correlations.")
```


# A2. Regression analyses for the SPI data set 

If you want to reload the packages, you can, but it's redundant if you're running the data all at once.

```{r, eval=FALSE}
library(psych) #make this package active
library(psychTools) #make this one active as well
```

## Score the items
Use the scales we found with `scoreItems` as predictors in the regressions. The criteria are the first 10 variables in the `spi` data set. 

```{r}
spi.scales <- scoreItems(spi.keys,spi) #find scores as well as scale statistics

sc.demos <- data.frame(spi[1:10],spi.scales$scores) #combine demographics and scores
#sc.demos <-cbind(spi[1:10],sc$scores) #combine with scores with demographics
```

## Split the sample
```{r}
set.seed(42) #for reproducible results

ss <- sample(1:nrow(sc.demos),nrow(sc.demos)/2)
```

## Find the multiple correlation
We use `setCor` to find the multiple correlations and beta weights. We do not plot the results.

```{r}
#derivation multiple Rs
sc.5 <- setCor(y=1:10,x=11:15, data=sc.demos[ss,], plot=FALSE)
sc.27 <- setCor(y=1:10,x=16:42, data=sc.demos[ss,], plot=FALSE)
sc.135 <- setCor(y=1:10,x=11:145,data=spi[ss,] ,plot=FALSE)
```

## Cross validate
Then do the cross validation, taking the second random half of the `spi` data.
```{r}
#now cross validate
cv.5 <- crossValidation(sc.5,sc.demos[-ss,])
cv.27 <- crossValidation(sc.27,sc.demos[-ss,])
cv.135 <- crossValidation(sc.135,spi[-ss,])
```

## Combine data from the two sets of participants

```{r}
# combine them into one data frame
cross.valid.df <- data.frame(cv5=cv.5$crossV, cv.27=cv.27$crossV, cv135=cv.135$crossV)
cross.valid.df.sorted <- dfOrder(cross.valid.df,1)
```

## Find the best items that predict the criteria

```{r}
bs <- bestScales(spi[ss,],criteria=colnames(spi)[1:10], folds=10, n.item=20,
                 dictionary=spi.dictionary,cut=.05)
bs.cv <- crossValidation(bs,spi[-ss,])
```

## Cross validate
Then do the cross validation, taking the second random half of the `spi` data.
```{r}
# sort them in ascending order of the Big 5 multiple R
cross.valid.df.bs <- cbind(cross.valid.df,bs=bs.cv$crossV)
cv.df.bs.sorted <- dfOrder(cross.valid.df.bs,1)
```

## Look at the outcomes

### Plot the outcome
This output corresponds with the results of Figure 1 from [Revelle, Dworak, &amp; Condon (2021)](https://doi.org/10.1016/j.paid.2020.109905). 

```{r}
par(pty="s")

matPlot(cv.df.bs.sorted[c(2,4,6,8)],minlength=8,
        main="Cross validation of multiple regression on spi data",
        xlas=3, ylab="Cross Validated R", pch=15:19)

legend(1,.6,cs(135,27,bestS,b5),lty=c(3,2,4,1),col=c(3,2,4,1))
```

Cross validated correlations predicting 10 different criteria. Four methods of prediction are shown. The Big 5, little 27 and the 135 item methods represent cross validated multiple regression models. The “best scales” method was found by adding up unit-weighted scores based upon those items identified using the `bestScales` function.


### Create a table of the outcome
```{r}
bs.spi.smoke <- bs$items$smoke
```

The following code generates latex code that we used in the manuscript. This is not helpful if you're trying to just look at the results.

```{r, eval = FALSE}
df2latex(bs.spi.smoke[c(2,3,5)])
```

To get around this, we can just print the results.

Zero order correlations of items from the `spi` that predict smoking and health. The composites of these items had cross validated predictive validities of .24 (smoking) and .44 (health). Items were identified using `bestScales`. Item numbers are from the list of all SAPA items. Mean correlations are the mean values across all 10 folds of the k-fold derivation samples. Standard deviations are the standard deviation across the 10 fold replications.

```{r}
# PURPLERABBIT: For some reason this table isn't matching up. Need to talk to Bill about this.
bs.spi.smoke[c(2,3,5)] %>% 
  mutate(mean.r = round(mean.r, 2),
         sd.r = round(sd.r, 2)) %>% 
  rename("mean r" = mean.r,
         "sd r" = sd.r, 
         "Item Content" = item) %>% 
  knitr::kable(caption = "Items predicting smoking form a scale with a cross validated prediction of .24")
```

We can do the same process with self-rated health. This code didn't appear in the manuscript, but it's part of Table 3.
```{r}
bs.spi.health <- bs$items$health

# PURPLERABBIT: For some reason this table isn't matching up. Need to talk to Bill about this.
# Also, highest value isn't .44
bs.spi.health[c(2,3,5)] %>% 
  mutate(mean.r = round(mean.r, 2),
         sd.r = round(sd.r, 2)) %>% 
  rename("mean r" = mean.r,
         "sd r" = sd.r, 
         "Item Content" = item) %>% 
  knitr::kable(caption = "Items predicting self-rated health form a scale with a cross validated prediction of .44") 
```

# A3. Graphical displays: The Manhattan plot
This output corresponds with the results of Figure 2 from [Revelle, Dworak, &amp; Condon (2021)](https://doi.org/10.1016/j.paid.2020.109905). 

```{r}
#Create the labels for the graph
labels <- names(spi.keys)
labels <- abbreviate(labels,minlength=8)

op <- par(mfrow=c(2,3)) #two row by three column display

man <- manhattan(spi,criteria=cs(health,exer,smoke),
                 keys=spi.keys,abs=FALSE,labels=labels)
man <- manhattan(spi,criteria=cs(health,exer,smoke),keys=spi.keys,abs=FALSE,
                 labels=labels,log.p = TRUE,main="")

op <- par(mfrow=c(1,1)) #put it back to the normal condition
```

A ‘Manhattan’ plot of the item correlations with three criteria: health, exercise, and smoking organized by scale. The top panel shows the raw correlations for the items, the bottom panels show the log of the probability of the correlations. Items are organized by those forming the Big 5 scales (the first five columns in each panel, and then items in the little 27.

# A4. Studies 3 and 4: Predicting 19 criteria from 696 items and scales 
Before you can do these analyses for the 126K cases in the bigger `sapa` data set you need to either get the data by [Condon and Revelle (2015)](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SD7SVE); [Condon, Roney, Revelle, 2017a](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TZJGAT), [Condon, Roney, Revelle, 2017b](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GU70EV) and getting the 3 `.rda` files there. We then stitch these three together using `rbind` to create the `full.sapa` data. If you do this, feel free to skip to the analyses code (see "Do the analyses"). Alternatively, you can use the code below (see "[Download the SAPA data from dataverse](http://127.0.0.1:38097/rmd_output/2/#pull-the-data-from-dataverse)") to download the data directly into R.  

## Download the SAPA data from dataverse 

###  Set up R for downloading the data 
```{r}
# Install the dataverse.
require(devtools) # You may need to run install.packages("devtools")
library(devtools)

# You will need to install dataverse if you do not already have it installed
# install_version("dataverse", version = "0.2.0", repos = "http://cran.us.r-project.org")

# Load the appropriate packages
library(psych)
library(data.table)
library(dataverse)
```

### Pull the data from dataverse 
We'll use the code below to the publicly-available SAPA data.

```{r, eval = FALSE}
# Load SAPA data from 08 Dec 2013 to 26 Jul 2014
dataset1 <- get_dataset("doi:10.7910/DVN/SD7SVE")
writeBin(get_file("sapaTempData696items08dec2013thru26jul2014.tab", 
                  "doi:10.7910/DVN/SD7SVE"), 
         "sapaTempData696items08dec2013thru26jul2014.tab")

sapaTempData696items08dec2013thru26jul2014 <- fread(
  "sapaTempData696items08dec2013thru26jul2014.tab", 
  na.strings=getOption("<NA>","NA"))

sapaTempData696items08dec2013thru26jul2014 <- as.data.frame(sapaTempData696items08dec2013thru26jul2014)

# Load SAPA data from 26 Jul 2014 to 22 Dec 2015
dataset2 <- get_dataset("doi:10.7910/DVN/GU70EV")
writeBin(get_file("sapaTempData696items26jul2014thru22dec2015.tab", 
                  "doi:10.7910/DVN/GU70EV"), 
         "sapaTempData696items26jul2014thru22dec2015.tab")

sapaTempData696items26jul2014thru22dec2015 <- fread(
  "sapaTempData696items26jul2014thru22dec2015.tab", 
  na.strings=getOption("<NA>","NA"))

sapaTempData696items26jul2014thru22dec2015 <- as.data.frame(sapaTempData696items26jul2014thru22dec2015)

# Load SAPA data from 22 Dec 2015 to 07 Feb 2017
dataset3 <- get_dataset("doi:10.7910/DVN/TZJGAT")
writeBin(get_file("sapaTempData696items22dec2015thru07feb2017.tab",
                  "doi:10.7910/DVN/TZJGAT"),
         "sapaTempData696items22dec2015thru07feb2017.tab")

sapaTempData696items22dec2015thru07feb2017 <- fread(
  "sapaTempData696items22dec2015thru07feb2017.tab", 
  na.strings=getOption("<NA>","NA"))

sapaTempData696items22dec2015thru07feb2017 <- as.data.frame(sapaTempData696items22dec2015thru07feb2017)


#Import "ItemInfo696" from dataset3 
writeBin(get_file("ItemInfo696.tab", "doi:10.7910/DVN/TZJGAT"), "ItemInfo696.tab")

sapa.icar.dictionary <- fread("ItemInfo696.tab", header=TRUE, na.strings=getOption("<NA>","NA"))

sapa.icar.dictionary <- as.data.frame(sapa.icar.dictionary)

# data.tables can't have rownames, so it shows up as variable V1. Give ItemInfo696 its rownames back.
rownames(sapa.icar.dictionary) <- sapa.icar.dictionary$V1

# Remove the now-superfluous first column.
sapa.icar.dictionary <- sapa.icar.dictionary[,-1]
```

### Remove Temporary Files
```{r, eval = FALSE}
#Delete the files created by the "writeBin" function.
file.remove(c("sapaTempData696items08dec2013thru26jul2014.tab",
              "sapaTempData696items22dec2015thru07feb2017.tab", 
              "sapaTempData696items26jul2014thru22dec2015.tab", 
              "ItemInfo696.tab"))

rm(dataset1, dataset2, dataset3)
```

### If you want, save the files to your working directory
```{r, eval = FALSE}
save(sapaTempData696items08dec2013thru26jul2014, sapaTempData696items26jul2014thru22dec2015, 
     sapaTempData696items22dec2015thru07feb2017, sapa.icar.dictionary, file = "sapa_data.rda")
```

```{r, echo = FALSE}
#load(file = "sapa_data.rda")
load(file = "sapaTempData696items08dec2013thru26jul2014.rdata")
load(file = "sapaTempData696items26jul2014thru22dec2015.rdata")
load(file = "sapaTempData696items22dec2015thru07feb2017.rdata")
load(file = "sapa.dictionary.rdata")
```


## Do the analyses

### Combine the three datasets and pull the scoring keys
```{r}
# get the files from Dataverse and then combine them
full.sapa <- rbind(sapaTempData696items08dec2013thru26jul2014,
                   sapaTempData696items26jul2014thru22dec2015,
                   sapaTempData696items22dec2015thru07feb2017)

sapa <- char2numeric(full.sapa) #convert string character data to numeric data
spi.items <- selectFromKeys(spi.keys)
```

### If you want, save the files to your working directory
```{r, eval = FALSE}
save(sapa, spi.items, file = "sapa_data.rda")
```

### Score the items
First, if you want you and if you are on a Mac, you can set the number of cores being used
```{r, eval = FALSE}
#set the number of multiple cores to take advantage of them
#does not work on PCs
options("mc.cores" = 4)
```

After deciding whether or not to change the number of cores you're using, you can find the scale scores and reliabilities.
```{r}
#find the scale scores and reliabililties
sapa.spi <- scoreItems(spi.keys,sapa,impute="none") 
spi.scores <- sapa.spi$scores
demos <- sapa[c(2:10,14:23)] #choose the 19 demographic data
demos.spi <- cbind(demos,spi.scores)
```

### Score the items

```{r, message = FALSE}
set.seed (42) # for reproducible results

ss <- sample(1:nrow(demos.spi),nrow(demos.spi)/2)
demos.b5 <- setCor(y=1:19,x=20:24,data=demos.spi[ss,],
                   plot=FALSE) #do the multiple regressions
summary(demos.b5)

demos.27 <- setCor(y=1:19,x=25:51,data=demos.spi[ss,],plot=FALSE)
demos.135 <-setCor(y =1:19,x=spi.items,data=sapa[ss,],plot=FALSE)

#now cross validate
pred.b5 <- predict.psych(demos.b5,data=demos.spi[-ss,] )
pred.27 <- predict.psych(demos.27,data=demos.spi[-ss,] )
cross.valid.b5 <- diag(cor2(pred.b5,demos.spi[-ss,1:19]))
cross.valid.l27 <- diag(cor2(pred.27,demos.spi[-ss,1:19]))
```

### Load in the data
This step may not be necessary if you have followed the code above to create the `sapa` object

```{r, eval = FALSE}
sapa <- read.file() #goes to my directory to find the file
load(sapa) #one extra step required
sapa <- char2numeric(sapa) #makes the fields numeric
```

### Score the data

```{r}
criteria <- colnames(sapa)[c(2:10,14:23)] #choose 19 criteria
spi.items <- selectFromKeys(spi.keys)
```

First, if you want you and if you are on a Mac, you can set the number of cores being used.
```{r, eval = FALSE}
options("mc.cores"=8) #I am using a mac with multiple cores
```

```{r}
scores <- scoreIrt.2pl(spi.keys,sapa) # do IRT scoring of the data

big.scores <- cbind(sapa[criteria],scores)
```

### Cross validate

```{r, message = FALSE}
set.seed(42) # for reproducible results
ss <- sample(1:nrow(big.scores),nrow(big.scores)/2)

#derivation multiple Rs
sc.5 <- setCor(y=criteria,x=20:24, data=big.scores[ss,], plot=FALSE)
sc.27 <- setCor(y=criteria,x=25:51, data=big.scores[ss,], plot=FALSE)
sc.135 <- setCor(y=criteria, x=spi.items,data=sapa[ss,] ,plot=FALSE)

#now cross validate
cv.5 <- crossValidation(sc.5,big.scores[-ss,])
cv.27 <- crossValidation(sc.27,big.scores[-ss,])
cv.135 <- crossValidation(sc.135,sapa[-ss,])

cross.valid.df <- data.frame(cv5=cv.5$crossV, cv.27=cv.27$crossV,
                             cv135=cv.135$crossV)
cross.valid.df.sorted <- dfOrder(cross.valid.df,1)

#ItemInfo is a dictionary of all of the sapa items
#this information is also on DataVerse
ItemInfo <- sapa.icar.dictionary
bs.sapa<- bestScales(sapa[ss,],criteria=criteria, folds=10, n.item=20,
                     dictionary=ItemInfo[,1:2],cut=.05)
bs.cv <- crossValidation(bs.sapa,sapa[-ss,])

#combine the best scales
cross.valid.df <- data.frame(cv5=cv.5$crossV, cv.27=cv.27$crossV,
                             cv135=cv.135$crossV, cvbs= bs.cv$crossV)

cross.valid.df.sorted <- dfOrder(cross.valid.df, 1)
```

### Plot the results
This output corresponds with the results of Figure 4 from [Revelle, Dworak, &amp; Condon (2021)](https://doi.org/10.1016/j.paid.2020.109905). 
```{r}
# PURPLERABBIT: For some reason this table isn't matching up. Need to talk to Bill about this.
# It appears that the order of the x axis is wrong
# When done with the dataverse data, it's missing bestS
par(pty="s")

matPlot(cross.valid.df.sorted[c(2,4,6,8)],
        main="Cross validation of multiple regression on sapa data",
        xlas=3, ylab="Cross Validated R",pch=15:18)

legend(1,.5,cs(bestS,27,135,b5),lty=c(4,2,3,1),col=c(4,2,3,1),pch=c(18,16,17,15))
```

Predicting 19 criteria from the SAPA set. The lines show the cross validated regression or fits for regressions using the Big 5 scores, the little 27 scores, and the 135 items from the `spi.` Also shown are the cross validated values for the solution the `bestScales` function.

### Plot the correlations
This output corresponds with the results of Figure 5 from [Revelle, Dworak, &amp; Condon (2021)](https://doi.org/10.1016/j.paid.2020.109905). 
```{r}
#now try profiles
R.big <- cor(sapa[ss,24:719],sapa[ss,criteria],use="pairwise")
R.pheno <- cor(sapa[ss,criteria],use="pairwise")
R.profile <- cor(R.big)
sapa.pheno.profile <- lowerUpper(R.pheno,R.profile)

par(pty="m")
corPlot(sapa.pheno.profile,xlas=3,main="Phenotypic (lower) and Profile (upper) correlations")
```

The phenotypic correlations (lower off diagonal) and persome profile correlations (upper off diagonal) of 19 criteria variables. The persome profiles are based upon 696 items. The profiles are based upon the correlations across 696 items and thus the standard error for these correlations are $\sigma_{r} = \sqrt{\frac{1-r^{2}}{694}} < .037$. The standard error of the phenotypic correlation based upon the sample size and the correlation and is < .003.

# A5. Study 5: Applying large sample profiles to a smaller sample

## Replace the criteria label to something more appropriate
```{r}
#we need to change sex to gender to the next step with the spi
colnames(spi)[2] <- "gender"
colnames(sc.demos)[2] <-"gender"

# The code is breaking here and only pulling age
small.crit <- criteria[criteria %in% colnames(spi)[1:10]] 
R.spi.sapa <- cor(sapa[spi.items],sapa[small.crit],use="pairwise") #these are the profiles

small.5 <- setCor(y=small.crit,x = 20:24, data=big.scores, plot=FALSE)
small.27 <- setCor(y=small.crit,x = 25:51, data=big.scores, plot=FALSE)
small.135 <- setCor(y=small.crit,x=spi.items,data=sapa,plot=FALSE)
small.prof <- R.spi.sapa
small.bs <- bestScales(sapa[c(small.crit,spi.items)],criteria=small.crit, folds=10,
                       n.item=20,dictionary=ItemInfo[,1:2],cut=.05)
```

## Validation with SPI data
For the small criteria,find the Big 5, little 27 and 135 regressions and best scales. We can can do this for the full sample since we are applying to different sample.
```{r}
#now validate with the spi data
valid.5 <- crossValidation(small.5, sc.demos)
valid.27 <- crossValidation(small.27, sc.demos)
valid.135 <- crossValidation(small.135, spi)
valid.bs <- crossValidation(small.bs, spi)
small.prof.valid <- crossValidation(small.prof, spi)
small.valid.df <- data.frame(cv5=valid.5$crossV, 
                             cv.27=valid.27$crossV,
                             cv135=valid.135$crossV,
                             cvbs= valid.bs$crossV,
                             profile = small.prof.valid$crossV)
small.valid.df.sorted <- dfOrder(small.valid.df,1)
```

## Plot the results
This output corresponds with the results of Figure 6 from [Revelle, Dworak, &amp; Condon (2021)](https://doi.org/10.1016/j.paid.2020.109905). 

```{r}
par(pty="s")
# PURPLERABBIT: Currently this is only pulling age. There's either an error with criteria on line 482 or I messed up the graph by loading tidyverse
matPlot(small.valid.df.sorted[c(2,4,6,8,10)],
        main="Cross validation of multiple regression from SAPA on spi data",
        xlas=3, ylab="Cross Validated R",pch=15:19)
legend(1,.5,cs(bestS, profile, 27, 135, b5), lty=c(4, 5, 2, 3, 1), col=c(4, 5, 2, 3, 1),
       pch=c(18,19,16,17,15))
```

Predicting 7 criteria for the `spi` data from the larger SAPA set. The lines show the cross validated regression or fits for regressions using the Big 5 scores, the little 27 scores, and the 135 items from the `spi`. Also shown are the values for the solution the `bestScales` function as well as the profile scores.


